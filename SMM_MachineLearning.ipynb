{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by Kaiwei Ang (Lance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SMM = pd.read_csv(\"SMM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID Number</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Mbr1</th>\n",
       "      <th>Mbr2</th>\n",
       "      <th>Discount</th>\n",
       "      <th>PayMthd</th>\n",
       "      <th>SalesChannel</th>\n",
       "      <th>CSI</th>\n",
       "      <th>Offer on 1st Ren Notice</th>\n",
       "      <th>NoEmail</th>\n",
       "      <th>...</th>\n",
       "      <th>Demo1</th>\n",
       "      <th>Number of key people on record</th>\n",
       "      <th>NumVst</th>\n",
       "      <th>Dinos Magazine in mail</th>\n",
       "      <th>Space Magazine in mail</th>\n",
       "      <th>Email notification about level price change</th>\n",
       "      <th>Mail notification about level price change</th>\n",
       "      <th>Eaddress acquisition postcard</th>\n",
       "      <th>Distance to SMM</th>\n",
       "      <th>Renew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236764</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.457016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298260</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.938433</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300288</td>\n",
       "      <td>364</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.951957</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300552</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.997106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>311620</td>\n",
       "      <td>364</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.612388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>314596</td>\n",
       "      <td>364</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.098156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>318040</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.692858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>319736</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.716185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>329464</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.909202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>331484</td>\n",
       "      <td>364</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.713390</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>340096</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.062926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>342776</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.166056</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID Number  Duration  Mbr1  Mbr2  Discount  PayMthd  SalesChannel  CSI  \\\n",
       "0      236764       364     1     2         1        1             1    0   \n",
       "1      298260       364     1     1         1        3             3    4   \n",
       "2      300288       364     2     1         1        2             1    0   \n",
       "3      300552       364     1     1         1        1             1    0   \n",
       "4      311620       364     2     1         1        1             1    0   \n",
       "5      314596       364     2     1         0        1             1    0   \n",
       "6      318040       364     1     1         1        1             1    0   \n",
       "7      319736       364     1     1         1        1             3    0   \n",
       "8      329464       364     1     1         0        1             1    0   \n",
       "9      331484       364     2     1         0        3             2    0   \n",
       "10     340096       364     1     1         1        1             3    0   \n",
       "11     342776       364     1     1         1        1             1    0   \n",
       "\n",
       "    Offer on 1st Ren Notice  NoEmail  ...    Demo1  \\\n",
       "0                         1        0  ...        1   \n",
       "1                         1        0  ...        6   \n",
       "2                         1        1  ...        1   \n",
       "3                         0        0  ...        8   \n",
       "4                         0        1  ...        1   \n",
       "5                         1        0  ...        1   \n",
       "6                         1        0  ...        3   \n",
       "7                         1        0  ...        3   \n",
       "8                         0        0  ...        3   \n",
       "9                         1        1  ...        1   \n",
       "10                        1        0  ...        5   \n",
       "11                        0        0  ...        7   \n",
       "\n",
       "    Number of key people on record  NumVst  Dinos Magazine in mail  \\\n",
       "0                                2       1                       0   \n",
       "1                                2       2                       0   \n",
       "2                                1       2                       0   \n",
       "3                                2       2                       1   \n",
       "4                                2       6                       1   \n",
       "5                                2       0                       0   \n",
       "6                                1       5                       0   \n",
       "7                                2       3                       0   \n",
       "8                                2       5                       0   \n",
       "9                                2       0                       0   \n",
       "10                               2       3                       0   \n",
       "11                               2       2                       0   \n",
       "\n",
       "    Space Magazine in mail  Email notification about level price change  \\\n",
       "0                        0                                            0   \n",
       "1                        1                                            0   \n",
       "2                        1                                            0   \n",
       "3                        1                                            0   \n",
       "4                        1                                            0   \n",
       "5                        1                                            0   \n",
       "6                        1                                            0   \n",
       "7                        1                                            0   \n",
       "8                        1                                            0   \n",
       "9                        1                                            0   \n",
       "10                       1                                            0   \n",
       "11                       1                                            0   \n",
       "\n",
       "    Mail notification about level price change  Eaddress acquisition postcard  \\\n",
       "0                                            0                              0   \n",
       "1                                            0                              0   \n",
       "2                                            0                              0   \n",
       "3                                            0                              1   \n",
       "4                                            0                              0   \n",
       "5                                            0                              0   \n",
       "6                                            0                              0   \n",
       "7                                            0                              0   \n",
       "8                                            0                              0   \n",
       "9                                            0                              0   \n",
       "10                                           0                              0   \n",
       "11                                           0                              0   \n",
       "\n",
       "    Distance to SMM  Renew  \n",
       "0          2.457016      1  \n",
       "1          3.938433      0  \n",
       "2         41.951957      0  \n",
       "3         20.997106      1  \n",
       "4         13.612388      0  \n",
       "5         15.098156      0  \n",
       "6         26.692858      0  \n",
       "7         24.716185      0  \n",
       "8         18.909202      0  \n",
       "9         11.713390    NaN  \n",
       "10        25.062926      0  \n",
       "11        14.166056      0  \n",
       "\n",
       "[12 rows x 23 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMM.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID Number</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Mbr1</th>\n",
       "      <th>Mbr2</th>\n",
       "      <th>Discount</th>\n",
       "      <th>PayMthd</th>\n",
       "      <th>SalesChannel</th>\n",
       "      <th>CSI</th>\n",
       "      <th>Offer on 1st Ren Notice</th>\n",
       "      <th>NoEmail</th>\n",
       "      <th>...</th>\n",
       "      <th>Demo1</th>\n",
       "      <th>Number of key people on record</th>\n",
       "      <th>NumVst</th>\n",
       "      <th>Dinos Magazine in mail</th>\n",
       "      <th>Space Magazine in mail</th>\n",
       "      <th>Email notification about level price change</th>\n",
       "      <th>Mail notification about level price change</th>\n",
       "      <th>Eaddress acquisition postcard</th>\n",
       "      <th>Distance to SMM</th>\n",
       "      <th>Renew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>9548.000000</td>\n",
       "      <td>8570.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6785267.817344</td>\n",
       "      <td>364.505760</td>\n",
       "      <td>1.208944</td>\n",
       "      <td>1.080121</td>\n",
       "      <td>0.317868</td>\n",
       "      <td>2.121177</td>\n",
       "      <td>1.383745</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.713553</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>...</td>\n",
       "      <td>3.312107</td>\n",
       "      <td>1.756284</td>\n",
       "      <td>2.719732</td>\n",
       "      <td>0.173544</td>\n",
       "      <td>0.894219</td>\n",
       "      <td>0.037495</td>\n",
       "      <td>0.005341</td>\n",
       "      <td>0.058651</td>\n",
       "      <td>72.009127</td>\n",
       "      <td>0.116686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>714477.091504</td>\n",
       "      <td>10.854218</td>\n",
       "      <td>0.406576</td>\n",
       "      <td>0.271495</td>\n",
       "      <td>0.465672</td>\n",
       "      <td>1.603873</td>\n",
       "      <td>0.669086</td>\n",
       "      <td>1.096795</td>\n",
       "      <td>0.452125</td>\n",
       "      <td>0.360526</td>\n",
       "      <td>...</td>\n",
       "      <td>1.771837</td>\n",
       "      <td>0.429346</td>\n",
       "      <td>2.063565</td>\n",
       "      <td>0.378737</td>\n",
       "      <td>0.307574</td>\n",
       "      <td>0.189981</td>\n",
       "      <td>0.072893</td>\n",
       "      <td>0.234983</td>\n",
       "      <td>226.924006</td>\n",
       "      <td>0.321065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>236764.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.145862</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6883117.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.936759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6937310.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.290407</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6990212.000000</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.953869</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7401876.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3968.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID Number     Duration         Mbr1         Mbr2     Discount  \\\n",
       "count     9548.000000  9548.000000  9548.000000  9548.000000  9548.000000   \n",
       "mean   6785267.817344   364.505760     1.208944     1.080121     0.317868   \n",
       "std     714477.091504    10.854218     0.406576     0.271495     0.465672   \n",
       "min     236764.000000   364.000000     1.000000     1.000000     0.000000   \n",
       "25%    6883117.000000   364.000000     1.000000     1.000000     0.000000   \n",
       "50%    6937310.000000   364.000000     1.000000     1.000000     0.000000   \n",
       "75%    6990212.000000   364.000000     1.000000     1.000000     1.000000   \n",
       "max    7401876.000000   729.000000     2.000000     2.000000     1.000000   \n",
       "\n",
       "           PayMthd  SalesChannel          CSI  Offer on 1st Ren Notice  \\\n",
       "count  9548.000000   9548.000000  9548.000000              9548.000000   \n",
       "mean      2.121177      1.383745     0.389610                 0.713553   \n",
       "std       1.603873      0.669086     1.096795                 0.452125   \n",
       "min       1.000000      1.000000     0.000000                 0.000000   \n",
       "25%       1.000000      1.000000     0.000000                 0.000000   \n",
       "50%       1.000000      1.000000     0.000000                 1.000000   \n",
       "75%       3.000000      2.000000     0.000000                 1.000000   \n",
       "max       8.000000      4.000000     5.000000                 1.000000   \n",
       "\n",
       "           NoEmail     ...             Demo1  Number of key people on record  \\\n",
       "count  9548.000000     ...       9548.000000                     9548.000000   \n",
       "mean      0.153540     ...          3.312107                        1.756284   \n",
       "std       0.360526     ...          1.771837                        0.429346   \n",
       "min       0.000000     ...          1.000000                        1.000000   \n",
       "25%       0.000000     ...          2.000000                        2.000000   \n",
       "50%       0.000000     ...          3.000000                        2.000000   \n",
       "75%       0.000000     ...          4.000000                        2.000000   \n",
       "max       1.000000     ...         12.000000                        2.000000   \n",
       "\n",
       "            NumVst  Dinos Magazine in mail  Space Magazine in mail  \\\n",
       "count  9548.000000             9548.000000             9548.000000   \n",
       "mean      2.719732                0.173544                0.894219   \n",
       "std       2.063565                0.378737                0.307574   \n",
       "min       0.000000                0.000000                0.000000   \n",
       "25%       1.000000                0.000000                1.000000   \n",
       "50%       2.000000                0.000000                1.000000   \n",
       "75%       4.000000                0.000000                1.000000   \n",
       "max      34.000000                1.000000                1.000000   \n",
       "\n",
       "       Email notification about level price change  \\\n",
       "count                                  9548.000000   \n",
       "mean                                      0.037495   \n",
       "std                                       0.189981   \n",
       "min                                       0.000000   \n",
       "25%                                       0.000000   \n",
       "50%                                       0.000000   \n",
       "75%                                       0.000000   \n",
       "max                                       1.000000   \n",
       "\n",
       "       Mail notification about level price change  \\\n",
       "count                                 9548.000000   \n",
       "mean                                     0.005341   \n",
       "std                                      0.072893   \n",
       "min                                      0.000000   \n",
       "25%                                      0.000000   \n",
       "50%                                      0.000000   \n",
       "75%                                      0.000000   \n",
       "max                                      1.000000   \n",
       "\n",
       "       Eaddress acquisition postcard  Distance to SMM        Renew  \n",
       "count                    9548.000000      9548.000000  8570.000000  \n",
       "mean                        0.058651        72.009127     0.116686  \n",
       "std                         0.234983       226.924006     0.321065  \n",
       "min                         0.000000         1.145862     0.000000  \n",
       "25%                         0.000000        13.936759     0.000000  \n",
       "50%                         0.000000        21.290407     0.000000  \n",
       "75%                         0.000000        37.953869     0.000000  \n",
       "max                         1.000000      3968.000000     1.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMM.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8570,)\n",
      "(8570, 21)\n",
      "(978, 21)\n",
      "(8570,)\n"
     ]
    }
   ],
   "source": [
    "#Constructing Train and Test set\n",
    "cmpr = pd.isnull(SMM[\"Renew\"])\n",
    "print(cmpr[cmpr==0].shape)\n",
    "TrainSMM = SMM[cmpr==0][SMM.columns[1:-1]]\n",
    "TestSMM = SMM[cmpr==1][SMM.columns[1:-1]]\n",
    "train_label = SMM[cmpr==0][\"Renew\"]\n",
    "print(TrainSMM.shape)\n",
    "print(TestSMM.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Something to keep in mind is, getting high accuracy from classifiers doesn't mean much here, it's likely to be a poor model since we have very little positive samples if comapred to negative samples. That is, even I now predict every single member will not renew (negative), I'll still get very high accuracy, which is ~88.3%. Hence, in order to determine how a model performs, we need to know its recall rate, precision, F1 score as well as AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Attempt: Direct Use of Random Forest Classifier With Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KaiweiAng/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88313506  0.88340336  0.88340336]\n",
      "Accuracy: 0.88331392739\n",
      "[ 0.        0.003003  0.003003]\n",
      "Recall: 0.002002002002\n",
      "[ 0.   0.5  0.5]\n",
      "Precision: 0.333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "pred = cross_validation.cross_val_score(alg, TrainSMM, train_label, cv=3)\n",
    "recall = cross_validation.cross_val_score(alg, TrainSMM, train_label, cv=3, scoring='recall')\n",
    "prec = cross_validation.cross_val_score(alg, TrainSMM, train_label, cv=3, scoring='precision')\n",
    "\n",
    "print(pred)\n",
    "print(\"Accuracy:\", pred.mean())\n",
    "print(recall)\n",
    "print(\"Recall:\", recall.mean())\n",
    "print(prec)\n",
    "print(\"Precision:\", prec.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the recall/true positive rate is very poor. The model literally predicted almost all the members will not renew. This leads to high accuracy but actually misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of positive and negative set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2857 8569\n",
      "0 2856\n",
      "630 5083\n",
      "370 2487\n",
      "-----------------------\n",
      "0 8569\n",
      "2857 5713\n",
      "754 4959\n",
      "246 2611\n",
      "-----------------------\n",
      "0 5713\n",
      "5714 8569\n",
      "616 5098\n",
      "384 2472\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "Train = TrainSMM.reset_index(drop=True)\n",
    "Train_Labels = train_label.reset_index(drop=True)\n",
    "\n",
    "kf = KFold(Train.shape[0], n_folds=3, random_state=1)\n",
    "for train, test in kf:\n",
    "    tr = Train_Labels.loc[train]\n",
    "    te = Train_Labels.loc[test]\n",
    "    print(train.min(), train.max())\n",
    "    print(test.min(), test.max())\n",
    "    print(sum(tr == 1),sum(tr == 0))\n",
    "    print(sum(te == 1),sum(te == 0))\n",
    "    print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the positive and negative set are quite evenly distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of Cross-Validation, Here's Another Approach: Randomly Select N Samples For Training. \n",
    "The very poor results we get earlier is due to the class imbalance. The ratio of number of positive samples to negative samples is around 1:8, which suffers heavy class imbalance. When class imbalance occurs, the model tends to assign the samples to the majority class aka abundant class. This's why the earlier model assign almost every single sample to negative label. So in this approach, we select equal amount of n samples from each class using random sampling method to avoid imbalance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of pos: (1000, 21)\n",
      "size of neg: (7570, 21)\n",
      "size of train_pos_idx: 800\n",
      "size of train_neg_idx: 800\n",
      "size of test_pos_idx: 200\n",
      "size of test_neg_idx 6770\n",
      "total # train set: 1600\n",
      "total # test set: 6970\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "pos = Train[Train_Labels==1]\n",
    "neg = Train[Train_Labels==0]\n",
    "print(\"size of pos:\",pos.shape)\n",
    "print(\"size of neg:\",neg.shape)\n",
    "\n",
    "train_pos_idx = random.sample(list(pos.index),800)\n",
    "train_neg_idx = random.sample(list(neg.index),800)\n",
    "print(\"size of train_pos_idx:\",len(train_pos_idx))\n",
    "print(\"size of train_neg_idx:\",len(train_neg_idx))\n",
    "\n",
    "test_pos_idx = list(set(list(pos.index)) - set(train_pos_idx))\n",
    "test_neg_idx = list(set(list(neg.index)) - set(train_neg_idx))\n",
    "print(\"size of test_pos_idx:\",len(test_pos_idx))\n",
    "print(\"size of test_neg_idx\",len(test_neg_idx))\n",
    "\n",
    "train_idx = train_pos_idx + train_neg_idx\n",
    "test_idx = test_pos_idx + test_neg_idx\n",
    "print(\"total # train set:\",len(train_idx))\n",
    "print(\"total # test set:\",len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6970,)\n",
      "[ 0.  1.]\n",
      "Accuracy: 0.626398852224\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "alg.fit(Train.loc[train_idx], Train_Labels.loc[train_idx])\n",
    "pred = alg.predict(Train.loc[test_idx])\n",
    "print(pred.shape)\n",
    "print(np.unique(pred))\n",
    "cmpr = pred == Train_Labels.loc[test_idx]\n",
    "print(\"Accuracy:\",sum(cmpr)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,) (6770,)\n",
      "6970\n",
      "(200,) (6770,)\n",
      "TP rate: 0.575\n",
      "TN rate: 0.627917282127\n",
      "Precision: 0.0436598329537\n",
      "F1 score: 0.0811573747354\n",
      "Confusion matrix:\n",
      "[[ 115   85]\n",
      " [2519 4251]]\n"
     ]
    }
   ],
   "source": [
    "Test_Labels = Train_Labels.loc[test_idx]\n",
    "pos_label = Test_Labels[Test_Labels==1]\n",
    "neg_label = Test_Labels[Test_Labels==0]\n",
    "print(pos_label.shape, neg_label.shape)\n",
    "print(sum(Test_Labels.index == test_idx))\n",
    "\n",
    "pred_df = pd.Series(pred, index=Test_Labels.index) #convert numpy array to pandas series \n",
    "pos_pred = pred_df[pos_label.index]\n",
    "neg_pred = pred_df[neg_label.index]\n",
    "print(pos_pred.shape, neg_pred.shape)\n",
    "\n",
    "TP = sum(pos_pred == pos_label)/pos_label.shape[0]\n",
    "TN = sum(neg_pred == neg_label)/neg_label.shape[0]\n",
    "Prec = sum(pos_pred == pos_label)/(sum(pos_pred == pos_label)+sum(neg_pred != neg_label))\n",
    "FM = 2*Prec*TP/(Prec+TP) #F measure aka F1 score\n",
    "CM = np.matrix([[sum(pos_pred == pos_label), sum(pos_pred != pos_label)], \n",
    "               [sum(neg_pred != neg_label), sum(neg_pred == neg_label)]])\n",
    "\n",
    "print(\"TP rate:\",TP)\n",
    "print(\"TN rate:\", TN)\n",
    "print(\"Precision:\", Prec)\n",
    "print(\"F1 score:\", FM)\n",
    "print(\"Confusion matrix:\")\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to check the consistency of the model's performance. To do so, we create a function, Classifier that does what the previous 3 cells do. Then, we run it 10 times and record the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n = number of training sample from pos and neg set\n",
    "def Classifier(n): \n",
    "    train_pos_idx = random.sample(list(pos.index),800)\n",
    "    train_neg_idx = random.sample(list(neg.index),800)\n",
    "    test_pos_idx = list(set(list(pos.index)) - set(train_pos_idx))\n",
    "    test_neg_idx = list(set(list(neg.index)) - set(train_neg_idx))\n",
    "    train_idx = train_pos_idx + train_neg_idx\n",
    "    test_idx = test_pos_idx + test_neg_idx\n",
    "    \n",
    "    alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "    alg.fit(Train.loc[train_idx], Train_Labels.loc[train_idx])\n",
    "    pred = alg.predict(Train.loc[test_idx])\n",
    "    cmpr = pred == Train_Labels.loc[test_idx]\n",
    "    ACC = sum(cmpr)/len(pred)\n",
    "    \n",
    "    Test_Labels = Train_Labels.loc[test_idx]\n",
    "    pos_label = Test_Labels[Test_Labels==1]\n",
    "    neg_label = Test_Labels[Test_Labels==0]\n",
    "\n",
    "    pred_df = pd.Series(pred, index=Test_Labels.index) #convert numpy array to pandas series \n",
    "    pos_pred = pred_df[pos_label.index]\n",
    "    neg_pred = pred_df[neg_label.index]\n",
    "\n",
    "    TP = sum(pos_pred == pos_label)/pos_label.shape[0]\n",
    "    TN = sum(neg_pred == neg_label)/neg_label.shape[0]\n",
    "    Prec = sum(pos_pred == pos_label)/(sum(pos_pred == pos_label)+sum(neg_pred != neg_label))\n",
    "    FM = 2*Prec*TP/(Prec+TP) #F measure aka F1 score\n",
    "    CM = np.matrix([[sum(pos_pred == pos_label), sum(pos_pred != pos_label)], \n",
    "                   [sum(neg_pred != neg_label), sum(neg_pred == neg_label)]])\n",
    "    \n",
    "    return TP,TN,Prec,FM,ACC,CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of TP rate: 0.6135\n",
      "average of TN rate: 0.610605612999\n",
      "average of Precison: 0.0444844835665\n",
      "average of F1 score: 0.0829509307393\n",
      "average of Accuracy: 0.61068866571\n"
     ]
    }
   ],
   "source": [
    "#running the Classifier function multiple times to check the performance consistency \n",
    "TP_list = []\n",
    "TN_list = []  \n",
    "Prec_list = []\n",
    "FM_list = []\n",
    "ACC_list = []\n",
    "for i in range(10):\n",
    "    TP,TN,Prec,FM,acc,cm = Classifier(800)\n",
    "    TP_list.append(TP)\n",
    "    TN_list.append(TN)\n",
    "    Prec_list.append(Prec)\n",
    "    FM_list.append(FM)\n",
    "    ACC_list.append(acc)\n",
    "\n",
    "print(\"average of TP rate:\", np.mean(TP_list))\n",
    "print(\"average of TN rate:\", np.mean(TN_list))\n",
    "print(\"average of Precison:\", np.mean(Prec_list))\n",
    "print(\"average of F1 score:\", np.mean(FM_list))\n",
    "print(\"average of Accuracy:\", np.mean(ACC_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In conclusion, the results are not great. The false positive rate is around 0.39, which is high (FP rate = 1 - TN rate). The F1 score is also very pathetic. To fix this, we need to pick a good decision threshold to reduce the false positive rate and increase the F1 score. Besides, we also need to check the area under the ROC curve (AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver operating characteristic (ROC) & Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of AUC: 0.6652\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "total=0\n",
    "for i in range(10):\n",
    "    train_pos_idx = random.sample(list(pos.index),800)\n",
    "    train_neg_idx = random.sample(list(neg.index),800)\n",
    "    test_pos_idx = list(set(list(pos.index)) - set(train_pos_idx))\n",
    "    test_neg_idx = list(set(list(neg.index)) - set(train_neg_idx))\n",
    "    train_idx = train_pos_idx + train_neg_idx\n",
    "    test_idx = test_pos_idx + test_neg_idx\n",
    "\n",
    "    alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "    alg.fit(Train.loc[train_idx], Train_Labels.loc[train_idx])\n",
    "    pred = alg.predict_proba(Train.loc[test_idx])[:,1]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Test_Labels, pred, pos_label=1)\n",
    "\n",
    "    total = total+metrics.auc(fpr,tpr)\n",
    "print(\"average of AUC:\",total/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best FM: 0.128695652174\n",
      "best precision: [ 0.07789474]\n",
      "best recall: [ 0.37]\n",
      "best FP: [ 0.12939439]\n",
      "best threshold: [ 0.62776688]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds2 = precision_recall_curve(Test_Labels, pred)\n",
    "FM = np.divide((2*np.multiply(precision,recall)), (precision+recall))\n",
    "FM = np.nan_to_num(FM)\n",
    "maxFM = FM.max()\n",
    "print(\"best FM:\",maxFM)\n",
    "print(\"best precision:\",precision[FM==maxFM])\n",
    "print(\"best recall:\",recall[FM==maxFM])\n",
    "FP = recall[FM==maxFM]*len(test_pos_idx)/precision[FM==maxFM] - recall[FM==maxFM]*len(test_pos_idx)\n",
    "print(\"best FP:\", FP/len(test_neg_idx))\n",
    "thresholds2 = np.concatenate((thresholds2, np.array([0])), axis=0)\n",
    "print(\"best threshold:\",thresholds2[FM==maxFM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As you can see, the performance is better when using a good decision threshold. 0.62776688 is the best threshold here. The only trade-off here  is the recall rate, which perform worse. However, this is good because the false positive rate drops 3 times while the recall rate only drops 1.6 times. This makes sense because the AUC is higher than 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Attempt: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88208537  0.88235294  0.8837535 ]\n",
      "Accuracy: 0.882730605655\n",
      "[ 0.00898204  0.00600601  0.01501502]\n",
      "Recall: 0.0100010189831\n",
      "[ 0.33333333  0.28571429  0.55555556]\n",
      "Precision: 0.391534391534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "alg = LogisticRegression()\n",
    "pred = cross_validation.cross_val_score(alg, TrainSMM, train_label, cv=3)\n",
    "recall = cross_validation.cross_val_score(alg, TrainSMM, train_label, cv=3, scoring='recall')\n",
    "prec = cross_validation.cross_val_score(alg, TrainSMM, train_label, cv=3, scoring='precision')\n",
    "\n",
    "print(pred)\n",
    "print(\"Accuracy:\", pred.mean())\n",
    "print(recall)\n",
    "print(\"Recall:\", recall.mean())\n",
    "print(prec)\n",
    "print(\"Precision:\", prec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Logistic_Regression(n): \n",
    "    train_pos_idx = random.sample(list(pos.index),800)\n",
    "    train_neg_idx = random.sample(list(neg.index),800)\n",
    "    test_pos_idx = list(set(list(pos.index)) - set(train_pos_idx))\n",
    "    test_neg_idx = list(set(list(neg.index)) - set(train_neg_idx))\n",
    "    train_idx = train_pos_idx + train_neg_idx\n",
    "    test_idx = test_pos_idx + test_neg_idx\n",
    "    \n",
    "    alg = LogisticRegression()\n",
    "    cols = [\"Duration\",\"Distance to SMM\"]\n",
    "    tr = Train.loc[train_idx]\n",
    "    mu = np.mean(tr[cols],axis=0)\n",
    "    std = np.std(tr[cols],axis=0)\n",
    "    tr[cols] = (tr[cols]-mu)/std #Z-Normalization\n",
    "    alg.fit(tr, Train_Labels.loc[train_idx])\n",
    "    \n",
    "    te = Train.loc[test_idx]\n",
    "    te[cols] = (te[cols]-mu)/std #Z-Normalization\n",
    "    pred = alg.predict(te)\n",
    "    cmpr = pred == Train_Labels.loc[test_idx]\n",
    "    ACC = sum(cmpr)/len(pred)\n",
    "    \n",
    "    Test_Labels = Train_Labels.loc[test_idx]\n",
    "    pos_label = Test_Labels[Test_Labels==1]\n",
    "    neg_label = Test_Labels[Test_Labels==0]\n",
    "\n",
    "    pred_df = pd.Series(pred, index=Test_Labels.index) #convert numpy array to pandas series \n",
    "    pos_pred = pred_df[pos_label.index]\n",
    "    neg_pred = pred_df[neg_label.index]\n",
    "\n",
    "    TP = sum(pos_pred == pos_label)/pos_label.shape[0]\n",
    "    TN = sum(neg_pred == neg_label)/neg_label.shape[0]\n",
    "    Prec = sum(pos_pred == pos_label)/(sum(pos_pred == pos_label)+sum(neg_pred != neg_label))\n",
    "    FM = 2*Prec*TP/(Prec+TP) #F measure aka F1 score\n",
    "    CM = np.matrix([[sum(pos_pred == pos_label), sum(pos_pred != pos_label)], \n",
    "                   [sum(neg_pred != neg_label), sum(neg_pred == neg_label)]])\n",
    "    \n",
    "    return TP,TN,Prec,FM,ACC,CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of TP rate: 0.6\n",
      "average of TN rate: 0.629556868538\n",
      "average of Precison: 0.0457019186099\n",
      "average of F1 score: 0.0849245130222\n",
      "average of Accuracy: 0.628708751793\n"
     ]
    }
   ],
   "source": [
    "#running the Classifier function multiple times to check the performance consistency \n",
    "TP_list = []\n",
    "TN_list = []  \n",
    "Prec_list = []\n",
    "FM_list = []\n",
    "ACC_list = []\n",
    "for i in range(10):\n",
    "    TP,TN,Prec,FM,acc,cm = Logistic_Regression(800)\n",
    "    TP_list.append(TP)\n",
    "    TN_list.append(TN)\n",
    "    Prec_list.append(Prec)\n",
    "    FM_list.append(FM)\n",
    "    ACC_list.append(acc)\n",
    "\n",
    "print(\"average of TP rate:\", np.mean(TP_list))\n",
    "print(\"average of TN rate:\", np.mean(TN_list))\n",
    "print(\"average of Precison:\", np.mean(Prec_list))\n",
    "print(\"average of F1 score:\", np.mean(FM_list))\n",
    "print(\"average of Accuracy:\", np.mean(ACC_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like Logistic Regression's performance is about the same as Random Forest's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding The Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAD7CAYAAACBvyWBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXm8VVX5/98fcEAhETVBSzHLIXNAFOcBp36VZU6khmlW\naqOaNpuB5tc5h9Q003BKTct5xBQFUROZcSAth8rAzAlxBJ7fH2tt7r7nnnPuwB2A+3m/Xvd191l7\nDc/aGzgPz1rr8ygiMMYYY4xZ2unR1QYYY4wxxnQGdnqMMcYY0y2w02OMMcaYboGdHmOMMcZ0C+z0\nGGOMMaZbYKfHGGOMMd2CZbraAGO6O5KsG2GMMW0gItSa+o70GLMYEBFL7c+IESO63AbPz/PrbnPr\nDvNrC3Z6jDHGGNMtsNNjjDHGmG6BnR5jTIcydOjQrjahQ/H8llyW5rnB0j+/tqC2rosZY9oHSeG/\nh8YY0zokEd7IbIwxxhjTFDs9xhhjjOkW2OkxxhhjTLfATo8xxhhjugV2eowxxhjTLbDTY4wxxphu\ngZ0eY4wxxnQL7PQYY4wxpltgp8cYY4wx3QI7PcYYY4zpFtjpMcYYY0y3wE6PMcYYY7oFdnqMMcYY\n0y2w02OMMcaYboGdHmOMMcZ0C+z0GGOMMaZbYKfHGGOMMd0COz3GGGOM6RbY6THGGGNMt8BOjzHG\nGGO6BXZ6jDHGGNMtsNNjjDHGmG6BnR5jjDHGdAvs9BhjjDGmW2CnxxhjjDHdAjs9xhhjjOkW2Okx\nxhhjTLfATo8xxhhjugV2eowxxhjTLbDTY4wxxphugZ0eY4wxxnQLlulqA4wxIKlLxu3ffyCzZj3f\nJWMbY0xno4joahuM6dZICuiqv4fC/wYYY5ZEJBERrfofo5e3jDHGGNMtsNNjWoyk+ZImSZohabKk\nY0v3tpB0bp22AyUd1DmWNhm7r6RvtaHd8XmuU/O8h+TyByQ9X1H3Zklz8vVASQsknVS6v6qk9yX9\nehGnY4wxpo3Y6TGtYW5EDI6IjYE9gM9KGgkQERMj4pg6bT8GfLkTbKxGP+DbrWkgaRvgc8CgiNgM\n2B34Z74dwOuStst1+wIDaLxG9RywZ+nzMGBGm6w3xhjTLtjpMW0iIl4BjgC+CyBpZ0m3la4n5+jI\nREm9gVOBHXLZ0TkaMlbS4/lnm1LbMZJukPSUpKuKMSUNkTRe0hRJj0rqLamHpDMk/TWXH17F3FOB\ndfPYp+e+zpQ0PUdxvlSlzRrAKxExL8/31YiYVbp/HVBErvYFbqxo/zbwlKTB+fMBwPUte7rGGGM6\nAp/eMm0mIp7LTseHi6L8+zjg2xHxiKQVgXeBnwDHRcReAJJ6AbtHxPuSPgFcCwzJ7QcBGwGzgPE5\nojKB5GgMi4hJkvrkfr8OvB4RW0taLtcfHREvlEz9CfCpiBicx94X2DQiNpG0OjBB0oMRMbvUZjTw\nC0lPA/cBf4yIsaV53g9cIqkHcCBwOPDzikd0HXCQpJeBecBLwJqteMTGGGPaETs9ZlGptnN+PHCO\npD8AN0bEv6scyV4OuEDSIGA+sF7p3mMR8R8ASVOAdYA3gZciYhJARLyV738a2ETSsNx2pdxX2emp\nZAeSk0VEvCzpAZLDdXtRISLm5ijNjsCuwHWSfhIRV+Y5zwMeIjk8vSLiRTWeZAB3AycDs4E/1nhW\nma47sm6MMUsCDzzwAA888MAi9WGnx7QZSesC8yLiv+Xv+4g4XdLtpD0t47NjUsn3gVkRsamknsA7\npXvvla7n0/DntJpnIOB7EXHvokylWmGks9xjgbGSpgOHAFeWqvwRuAn4RdGkov08SROBY0mRqy/W\nMsDHxo0xpj5Dhw5l6NChCz+feOKJre7De3pMa1joHOQlrYuA85tUktaNiCci4gzSstSGwBxSFKag\nL/CffH0I0LOZsWcCAyRtkcfok52le4BvS1oml68naYWKtnOAD5U+jwMOKC3N7Qg8VjGH9fOyW8Eg\nKqJHETEOOIW0jAWNnafi+lfAjyPi9XqTk4QkBgxYp141Y4wxi4AjPaY19JI0ibQ09QFwZUScU6Xe\nMZJ2IUVpngDuIkVB5kuaDFwOXAjcKOkQ0jLQ3BpjBkBEfCDpANKS2AqkjcK7A5eSlr8m5eWll4G9\nG3UQ8WreAD0NuCsifixpW2AqsAD4YUS8XDFuH+D8fDJrHvAsaeP2Qpty32dX2lph95PAkzXm1qTp\n7Nlds8xljDHdASsyG9PFNFZktkKyMca0BCsydwAlQb7iCPaPWtF2YN4LUu3emNJx5iUaSSdK2rXO\n/SMlHZyvD5U0oHTvEkkbdoadLUHSTzuw76XmnRtjzJKIl7eaZ25x1LmNtOq/7ZJ6RMSCRRiv04mI\nEc3c/23p41dJIn2z8r0jqrXpQn5G0vVZJCT1jIj57WCPMcaYdsKRnuapGjqTdEIWxJsm6eJS+RZZ\nJG8y8J1SeS9J10p6QtKNQK/SvTmSzspttpE0WCnVwQRJd0nqn+sdldtPkXRNLqsmBFhp6025r+mS\nvlEq/0xuM1nSvblsFUn35Lq/k/R8LmsUtZJ0nKRf5OtRWfsGSacppW6YIumMXDYi198P2BK4Otvb\nqxz9kHRQfp7TJJ1W8XxOzn0+rAZdoPIcR0i6Mt+fWTHPJkKEkgZIejDbMU3S9pJOBVbIZVfleofk\ndpMlXZHLPq8kjjhR0ujCnpINDwFX5vldV+2dG2OM6QIiwj91fkibWCcBk/PvYbl85VKdK4E98/VU\nYPt8fQYwLV9/H7g0X29C2gg8OH9eAOyXr5ch6dysmj9/CbgsX/8bWDZfr5R/3wpsm69XBHpUmcPK\n+XcvYDopLcNqwIvA2hV1zgN+nq8/R9qMvAowsJhLvncc8It8PYqkSrwK8HSpTmHjCODYfD0G2LxU\nZwwwmKSA/ELuowdJEHCv0vP5XL4+HfhZlTmOyO9oOWDVPLcB2a57cp3V8xj9ScfIf5rLBfTO12+W\n+twIeBroV/GM+pbqfB04s2TDBGC55t55he0BkX8IY4wxzZP/vWzVd7qXt5rn7ai+vLWbpB+SHI1+\nwIz8P/y+ETE+17kK+Ey+3onkUBAR0yVNLfU1j4Y0BhsAGwP3ShLJAXgp35sKXCPpZuDmXNZECLCK\nrcdIKk40fZQk3rc68GBEvJhtKo5U7wTsk8vulPRanWdTyRvAO5IuBe6gJPZXQbXo2RBgTES8CpDn\nsxPJqXs/Iu7M9SaSTm1V45aIeB/4n6T7ga2pLUQ4Afi9pGVzu6lV+tsVuCEiXsvti2e0lqTrSY7a\nsqQ8WwW3Zhug/juvID2S3r378sADDzTSojDGGGNxwi5D0vKkI9eDI+IlSSNoWLpo6U7ycr13s9da\nlM+IiO2rtNmT9EW6F3C8pI2jihBgRPytZOvOpC/vrSPiPUljmrG1cg9SUWcejbV0mizVRMR8SVsB\nu5ESbH43X7eUWs/ug9J1WaywiQkVfVXbG6Vs6zhJO5Ke2+WSfhURV9exocz5wFkRcUd+vuU9TbWO\n3i8cu6rhUfnYjTHGlLE4YedQ7YuqF+kL9n9KOaD2B4iIN4DXlLNvAweX2owFhgNI2hjYtMYYM4EP\nqyEB5zKSNsr31o6IB0m5pFYC+qi6EGCZvsBr2eHZENgmlz8K7ChpYB6nXxU7PwusnMtnZ7v6Zafv\n800eVMqztXJE3E1aPtq0sg5NRQoLHgN2yvuHepKSeT5QpV49vihpOUmrAjuTnkdVIUJJawMvR8Rl\nJK2fIpr3fh4fUn6tYZJWyfMrntFKNETfDq1jT7133ghlcUJZpNAYYzoMR3qapxDkEzmfUkT8LC/h\nPEFSFS6r+X6NtGyygJS0suAiYJSkJ4CngMdL98pidx9I2p8GYbyewLmS/kbaALxStuW8iHgzb/Ct\nFAIsczfwzTzuTOCRPM4rko4AbsrLaC8D/w84CbhW0oHAw6S9MUVKhZNIjsS/8hwq7V8JuEUpmSik\nPS2VXA5cLOltYLuibUTMkvQTGhydOyKiWB5raRhkWm6/KnBSpKzoN2UHspEQoZIo4g8lfUByxA7J\nfVwCTJc0MSK+IukU4EFJ80h7hr4GnAj8SdKrJMdonRr21HvnFTSeokUKjTGm/bE4oamLpOeALYq9\nNosreYlxTjRWSF4iUCNxwoWlXvIyxpg6qLPFCSUtkHRl6XNPSf+VdGsz7baQdG6+PlRSk/xNbbCl\nr6RvlT6vkTebFp+vVTryfLSkkaojpldnjIGSDqo2j45C0nPF8soi9tPI9ir3qooo0kqdoRb22dI+\nOky4UBYKNMaYbseiLm/NBTaWtHxEvAfsAfyzuUYRMZF0Cmdh0SLaAekE1bdJSwpExH9Ix71RUgDe\nMiLWW8QxPgZ8mYbTQJXz6Aja67/7jWxv6TgRse4ijNlm25VEGlssXBgRrd/RZowxplvRHhuZ7ySd\ngIG0+XThl6qkIUpicRMlPSRpvVy+s6Tb6nWqJPR2Wf4f+bOSvle6d6yS2Nw0SUfl4lOBdZWE5U6v\niDTcA6yZ7+2gxmJ6Q5SSUU5REpzrnduOlfR4/tmmNMYOuZ+jy/PIG3xvUhKyezhvXK07j4r5/kbS\nY3le5dNAAn6c5/qopHVz/YGS7st23yvpo7l84dzy5znVbK/z3HtIOkNJeHGKpMNz+bVKG5uLeqMk\n7Vurfp3+d1YSBbxd0tOSflO2VQ0ijduqsXBhNSHFFfOzLYQCv1BjzOL5TVbao1PwpWz305K2Lz3X\nJu8+2z1G0g2SnlIWL8z3PpfLJkg6r/RnokX2GWOM6SRaK+xT/gHeJGnK3AAsT9rouRNJqwRSpuoe\n+Xo34E/5eudSnUOBX1fpewTwECkatSrwCmlT7xakTam9gN6klAab0VQ8b+HnKvcKMb1lgb/TIBLY\nh+QI9qJBYO4TwIRKu6vM49fACfl6F2ByvXlUmW8hfNeDJNi3cf78HPCTfP0V4LZoECU8OF8fBtxU\nnlv5HVWzvWLs8rM6nCz+RxL6m5Dv7w1cnsuXJYn8LV+nfqNnXvHM3s73RdrsvW9UiDRGY+HCWkKK\n/wd8OV/3JW3UXqFivM/k5798RdsxNIgKfha4NxoEHGu9+9dI2jwibfLeLj+Dsm3XlP5MNGtfNBEn\ntEihMca0BLpCnDAiZkhahxTluYPGx69XJsnxr5f+YW/1eHdExDzS0fDZJCXd7Ulf8O8CKMn77wjU\njRzVYAPgpYiYlOfyVu5zOeACSYNIp6Jasiy2A8mRIiLGKB297lNnHi9VtD8wR0mWISkJb0Ry6ACu\ny7+vBYqNutuSRQRJIoint2zKzfJpYBNJw/LnlUjzv4t0imxZkpMwNtIx+Fr1n6kzxmMR8QKkCBLp\n2d1IetY3Vqm/DdWFFD8NfEFJJBKS07U2ybko2B0YFWn5tdyW0lgTSU5Y0Uetd/9YpGVTJE0hndqa\nC/y9sI30jopoV0vsyzTei9e//8CmVYwxphuzOIkT3gqcCQwl/a+84JfA/RGxr5IezJhW9vte6bqe\nKN2iUG3n9/eBWRGxqZJmyzuLOEZ5HguomEd2Go8jnZJ6U9IoGov/RY3raswjL1tKEumLtjUI+F5E\n3NvkRlIz/gxwAA3LmFXr5/ddi8o5FJ/fyd57LbuqsV9E1HOw6lG8l/KfrXrvvtafx1q2qaX21Z62\nMcYYWDzECYt/7H8PnBgRT1Tc70vKFwVpCaY9xhoH7K2UzLE3KdoxjqS18qEabSqvC2YCAyRtASCp\nT/6i60vS34Gk31KI1VUbo2AcWYxQ0lDglSJy1AJWAt4C5iglF/1sxf0D8u8DyTo7pPQTxWmsg/P4\nAM+TknoCfJG0FNWc7WXuAb4taZk8l/UkrZDvXU96jzuQ9H+aq1/LGdgq753pkec2rpn6tYQU7wGK\nPV3k6Ewl9wKHFTaV2tai1ruvxUzgY0pih9DwrlpqX3HPwoTGGNPBLKrTUwjL/TsiLqhy/wzgNEkT\n23GsySSBuwkkB+CSiJgaSUdmfN6wenq5Ta3riPiA9CV1QV6uGE3ao/Eb4Kt5Q+36NKQWmAYsyBti\nKzcDjwS2UMqvdAoNYndV59GoIGIaMIUkYHc1aQ9KuX6/3O/3aBD8O4r0ZT6FpPpb2PM7YOds+zYt\ntL3MpcCTwCSljeAX0xDRGE3as3VvXq5rrn6t8MXjwAUkMcW/R8TNNeoX7+kVoBBSnEzDct/JwLL5\nnU8nCSs27iDiHlIk8nElkcnjmrGt1rtv0nXu/13SqcF7JE0g7XN7I9f5ZXP2Ne4u/cye/ULtasYY\nY9qMxQlNp6KUq+q4iNirq21pLyT1joi5+fpC4G8RcV4r2leIE1qY0BhjmkMdIU4oab7SMefJ+feP\n2m5io34fyr8XWcSu1OfRakiBgNKx6JXy9VGSnpR0laTPt3Uekn5a8fmhWnXbA1UcQV/Evn5a5167\niCC2Z5+SjpR0cPM129T3CEnHtlN3h+e/H0+Qlip/2079GmOMaUeajfRIejMiqiWIbB8D0j6N2yKi\nZjLGVvRVM2WCpKeA3SKi8tRUa8eYExEt2RvTLuRNzbdFRLVTTa3tq6btkv5BEnBst3QTi9KnpJ4R\nMb+9bKnS/2KTtsKRHmOMaT0dEumhxubS/L/4U/L/cB+TtLmkuyU9I+nIXKe3pL8oibxNlbRXqf2c\nav2W7tcTg9stR52mSrpUKbP294A1gTGS7ivZuIqki4B1gbtyNGhh6gtJq0u6UUlYb7IaxOhuUhKb\nmy7pG7nsVGCFPPZVlfOQdGauP1XSl5qbR8V8v5Gf4+Rct3x6a49sy9OS9sz1l5f0+7xfZKLS5ukm\naT0k3SZpp2q2V5pQajNcSbRvkqSLlAQIj5R0RqnOoZJ+XaO+KvusmOscSWdLmqEkrLhqLh8j6RxJ\njwFHlaMxkj6e607Jf54+lst/kJ/bFDUWdSyPVwgbTlEWNsx8StXFL5u8+5LdJ+d+HlbK2o6kdSU9\nkt/7Lyv+TDRrnzHGmE6iOSEf0hHoSSThwUnAsGgQzTsiX59N2oi7IunI+qxc3hPok69XBZ4p9VuI\n5tUTsasnBvfxXO8K4KiSTf1KffwDWKV03S8qBBFJm2KL9gI+FI1F7HoB00tt36yws5jHfsA9+Xp1\nknhf/1rzqDLfst2/BL4TDWKDd0aDWN4/ScfQjwUuzeUb5PGWo0LskaRftFM12yvGfw5YBdiQtPG3\nZy6/kHQ6bLWK93dnfh9V65f7rDLWAuDAfH1C6V2MAS4o1RsBHJuvHwX2igYRxF6ktCe/Lb2724Ad\nKsaqJWw4ghqikXXe/QLgc/n6dBpEGW8DvpSvjyz9mWjWvqgqTmhhQmOMaQ46SJzw7YiolZixEASc\nDvSOiLeBtyW9q7SX5m3gVEk75S+MNSWtHhEvt2BcqC4G9xbwj4j4e65zBen0zK/z51rH1EX1yMOu\nJKVj8kMs/pd+jKS98/VHSSJ1j9WxdXsacnK9rKRpMyT3V20eD1e030TSySRBx96k484F1+d+n5X0\nd+CTpGPjv87lMyU9Tzpt1FaK9ZTdSCrIE3LEphcwOyJekfR3SVsBzwIbRMTDkr5Tpf6sZsaaX8yJ\ndFrtz6V7f6ysrCTyuGZE3AoQEe/n8k+TomCTSO+2N+k9lfdZ1RI2hNqikbXe/XsRcWcun0gSPoQk\nFPnFfH0NSbMKkjhhc/YVs1x41a9f/6a3jTGmm7M4iBMWYm0LqC7AN5z0P+3NI2KB0p6bXrSc1orB\ntYUmmyeUThjtCmwdSXV4DA12t3Tscr2WiCxeTopkzJB0KClCVM1GkZ5vrfEWihNmelWpUw8BV0TE\n8VXuXUc64v80cFML6reU8vxqHRGvZruAUyPid830X2veTf7MNvPuPyjVL7/HyvfTWvu8h8cYY5qh\ns8QJ2+JgFG36Ai9nh2cXGqT+K/ttzRgzgYHKiTdJUZoH8vWbpNMzreE+UqSoSLa5Urb7tfyltyEp\nWlDwvrIQX4Xt44ADch8fJqXGqBcZqqQPMEspzcPwinvDlPg4KVv6zDze8Gz3+sBaufx5YFCuvxaw\nVYXttcT2inncB+xf2q/STw3CezeTIhoH0qCVU69+LXoC++fr4VSNfDQQSeTxn5K+mMdYTkls8B7g\na0oilUhas7CjRC1hw1rUe/f1xBOL+RxYKm+JfcYYYzqJljg9vdT4yHqRpbref02Le38AhigJ6x1M\nEt+rrNNcX43qRMqhdBjwp9zvfBqOCP8OuFt5I3MLxzgG2EXSNJJo3idJasPLKh1BPoUGFWSAS4Bp\natgMXNh1E0kAcCrwF+CHNZbxatlxAslJGkfj5wRpT8pjpNxmR+blnd8APbPd1wKHRsQHETGe5Pg8\nAZxLWoYp2z5d1TcyF/N4Cvg5MDo/39GkXGDF0tBTpP0xjzdXv85c55JUmaeTUpec1Ex9SGKPR+Ux\nxgP9I6W+uAZ4JD+HG0jOY8OkagsbVp0/9d99Lfu+Dxybly4/ThYnbIl9xhhjOg+LE5pOR5187L+j\nkbRCRLyTrw8gbdLep5lm5fbhv4fGGNM61EFH1k0XIul4paPdU3OkbUgX2HC5pLnFMk0uO1fSArVN\nfLBV3/CStpB0bhvGqdbXJXnZqj3ZIh9Jnwp8i4ZUF0g6UdKu+XqMpFqHAowxxnQwHZG13LQTSppB\nnwMGRcS87GC0Nmt6exDAM6T9PNfkU1q7AP9qU2etFLuMiIk0XqZrMxFxRHv0U9HnQ0DVZKIRYW0e\nY4xZTHCkZ/FmDVK29nkAEfFqRMyChcKLpyuJEz5abOxWSrHxqJIY3+jSBuPeahAznCJpn1y+h5LQ\n3uOS/ihpxRq2FCe3IO3DGU86KUbup5ag39clzcw2XaIGQcNadt5R2kP2uqSvKAk83pbvj5B0maqL\nCtYSSaRUZ2G0RTXEBivqj8iRrrH5me9Teu53Km8Ml3RCHnuapItL7dstjYgxxphFw07P4s1oYG0l\nJeYLlfSOyrwWKX3HhUCR4HJcRGwTEVuQNG+KHGMnAK9HxKYRMQi4X0kJ+eek9BxbkqIpx1GdZ4AP\nS1oZOIisSVTisIgYQtImOlrpFNcauf+tSDpG5WWlSjt/DBARe2ZdqK+TNmRXy8C+AUn4b2tghKSe\necnqAJLw42DSEfTKU3CV9AYezs9jHHB4jXrrkhy9L5J0he7Lz/1dYM9c5/yI2DqXr6isnG2MMWbx\nwctbizERMTdHJXYkacdcJ+knEXFlrlKcRLoWOCdfryXpelKUaFmSKjIkIb0iUkNEvJG/mDcCxueo\nyLI0Pq3UyBzgRtKR7K1IysPlSEo1Qb81gAci4g0ASTfk8np2Imk14Cpg/4iYUyVgU01UsKqoYo25\nFNQSG6zkriy7MB3oERGjc/l0ktAkwG6SfkhSJe8HzCCdtmsRI0eOXHhdqUVhjDFm8RAnNB1MPtYz\nFhibv3QPAQqnpxz9KAQLzwfOiog7lIT26u0pETA6IpqLiBRcT3IORkVEFM6I2ibmWNVOST1ITtzI\nfBy+GtXEHtsiklhLbLDqeHnO5TaFoOHypGjb4Ih4SSnHVmtEOBs5PcYYY5rSWeKEpouQtL6kT5SK\nBpFybBUUkZsDaYjQrERKpQApD1fBvcB3Sn2vTBLV215J9BBJK0pajxrkVA4/Ay6quFVL0G8CsJOk\nvkqCjvuV2tSy83RgakTcUMuOCloiqthc29ZQrU0vkgP6P6WUGftXqVO/U6nuz4AB67TBVGOMMWUc\n6Vm86QOcL6kvadPwsyShvYJ+Ssek3yXtswE4kSTc+CpwPw3LLycDF+Zo0TzgxIi4WdJXgWtztCJI\ne3CeqbBjYUSpIqVCWdDvm0qCfjPJDliOepxCElZ8lZS+4o1m7DwOmKEkJBjAL2jIh1aNhaKKkgqR\nxB7A+yQn78Vac6GVR+drtclLhZeSBCH/Q2Ml7haOV9+U2bPbM/OKMcZ0TyxOuIQgaT5J7Xk50rLM\n2sB6EfGqpC2Ar0TEMR009uXAMGD1iJiby84FjgJWi4hX67Ttnfcm9STl67osIm5p5fjtNj9JlwBn\nR8TTi9pXC8c7kZTw9P687HdcREyqqBPN+19yfi5jjCmhNogT2ulZQpD0ZqFvkzf6vkg6MfTjThh7\nFLA5cEZEFDo9U0gbdgc14/ScSdogvDxp/1CHOGZLAnZ6jDGm/WiL0+M9PUsgOZ/Up0jHummFjs2x\nSjo60yQdnctWlHS7ki7ONEnDagzbJp0e0pLWiqRksCvKOj3GGGO6CDs9SygR8RzQo/RF3ZyOzRak\nDcNDgG2BwyVtBnwG+HdEbJ41Zu6uMaR1eqzTY4wxSzTeyLxkUyusV03HZnvgpoh4F0DSjST9n3uA\nsySdmts9VKNP6/R0oE6PMcaYjsdOzxKKUtqJeRHx3ypOQTUdm6pExDN5uedzwMmS/hIRJ9eobp2e\nDtLpae70fP/+A1vXnTHGLGVYnLB7sfBbMS9pXURyHFrabhwwStJpQE9gH+DgvAT1at6g/AZ5n1A1\nIuJFST8D/lJxq55OzzlKR+7nknR6puV7HaHTc7Okc7Mj2A/4UNYWaq5ta2ipTk9L7QfwJmVjjGmG\n9hAntNOz5NBL0iQajqxfGRHnNNMGGnRsJisdPZ+Qyy6JiKmSPg2cKWkBSdvmW7X6yP1Yp6eyoB10\neqpE64wxZomjf/+BzJr1fFebURMfWTcditpBp2dpp2VH1o0xZkmg8+Q1fGTdLI6MzFGb6cA/OtPh\nkbRASSeo+HycpF+0Q787SXq4oqynpFmSBtRos7OkbRd1bGOMMW3Hy1umQ4mIH3bh8O8B+0o6tZ6A\nYhsYB3xE0loR8c9ctjswIyJm1WgzFHiL2lnsjTHGdDCO9JilmXnAJcCxlTcqRQMlzcm/d5b0gKSb\ns/jhqZK+nIUHp0r6WKTY7fWk4/sFB5K1iyQdJemJLHp4jaSBwDdJx/onSdq+46ZsjDGmFnZ6zNJM\nkI6SD5f0oRbULdiUlNh1I+ArpBxnWwOXAYUC9HXkJK+SliMd+f9TvvdjUnqOQcA3I+IF4GLgnIgY\nHBHjF3lmxhhjWo2dHrNUExFvAVcAR7ei2YSIeDki3gf+DjQRI4yIiUBvSesBnwUeLUQYSYlhr5E0\nnKT/Y4zg1UUAAAAgAElEQVQxZjHAe3pMd+A8YBIwqlQ2j+z0ZwXn5Ur3yuKHC0qfF9D478y1pGjP\nJ2mclmNPYCdgL+B4SRs3b6KPrBtjlnw6UkjV4oTG1EcAEfFaTnnxddISFaS8XluSlqS+SEqF0Vqu\nA24lCS1+DRY6UGtHxIP5hNcBQB+S1tBKtTqydIQxxtSnPcQJvbxllmbKnsSvgFVLZb8Dds7H6bch\nKUY310fjGxFPk05k3RcR7+TinsDVkqaSUnacFxFvArcB+9TayCyp0c+AAeu0eJLGGGNahsUJjeli\nqosTdp7AlzHGLIlYnLCFdJRoXe6r0VHojkLS/pKelHRfRfnOkm7r6PHbm856bl1Jd5ijMcYsznRL\np4cG0bpVutqQMjlVQ0v5OvCNiNityj2HCFqBOiDxVSvfpTHGmE6guzo9HSJaV+pmD0kTJD0tac/c\nvoekM3L9KZIOL/U7VtItpISVlfYcJGla/jk1l50A7ABcJun0WpOUNCTvIfmYpBUlXSbpUUkTJX0h\n13lQ0qalNuMkbVLRz6F53mMkzSxHxSQNz3OaJOmiwoGosPu08vOUdLakGZLulbRqFbsH52c9QdJd\nkvpXqTNQ0n35Wd4r6aOl93eepPH5PTWJrOS2T0u6QtJ04KOS9pD0sKTHJf1R0oqlZzg+j/OopN6S\nlpf0+zy3iZKGlp7TLTn69pdcdoGkpySNBlav9a6MMcZ0PN3V6elI0TqAgRExBPg8cLGSeN3Xgddz\n/a2AI5SUegE2B74XERuWB5a0BnAaKYXBIGArSXtFxC+Bx4EvR8SPqxmtlOfpN8AXIuI54HjShttt\ngF2BsyStAFwKHJbbrAcsHxHTq3Q5BNgH2AwYlh2TDUmnk7aLiMGkI93Dq9g9RNJeuZ/ewGMRsTEw\nFhhRYfcywPnAfvkZjgJOqWLP+cCoLAB4Tf5cMCAitge+ANRyCj8BXBARmwBvAz8HdouILUkbkI+V\ntCzphNb38ji7A++SsrcviIhNgS8DV+R3DOld7hsRu0jah/Rn5JPAocB2NWwxxhjTCXTbI+sR8Zak\nQrTunebqZyZExMsAkipF64aW6l2fx3g219sQ+DSwiaRhuc5KwHrAByQn4MUq4w0BxhR5oyT9gaT/\ncmu+X2tZZiPgt8CnS7mgPg18QVKRC2s5YG3Ske0TJP2AdOz68hp93hsRr2c7/kyKNM0HtgAm5AhP\nL2A28GYduxcUzwe4GvhzxTgbABsD9+Y+ewAvVbFnW5ITBnAVjZ2bmwEi4ilJtaIrL0TEhHy9DemZ\njc9jLkvKkbUB8FJETMr9vZXnswPw61w2U9LzwPql51SIFO5E1u+JiP9Iur+GLcYYYzqBbuv0ZDpK\ntK4cHVL+LFLE4N6yAZJ2pvZx6aJ9a/kPsDwwGLizVL5fRDzTZADpXmBvYBjJialGtTkBXB4Rx1f0\nt1cr7G56bCkl7mwuP1W9fUvl91TLjrkVdUZHxPBGhiRRwZbMo1yn3rtsYRcdK/BljDFLIu0hTthd\nl7cWitaRog5fL917niRaB20XrRumxMeBjwEzgXuAb+flGyStV+wbqcNjwE6SVlHaGHsQ8EALxn+N\npAp8qqSdctlo4KiigqRBpfqXkSIXj5WiFJXsIWnlvCS2NzAeuB/YX9KHc5/9JK3djN09gP3z9XDg\noYpxZgIflrRN7nMZSRtVsefh3C/AwaTM59Wo5bSUyx8Fts/vC6X9T+tlWwZI2iKX98nzGZdtR9L6\nwFq5biVjgQOU9nOtAexSwxYiotHPrFnP16pqjDHdkqFDhzJy5MiFP22huzo9HSpaB7xI+uK/Azgy\n53C6FHgSmJQ3z15MErKrbWRamvoJyWGYTFpeu70F4xMR/yXtKbpQ0hDgl8CyefPtdOCkUt1JpCWp\nUVU7SzwG3AhMAW6IiEkR8RRpL8xoJTG+0aT9NJV2P16yey5pb1KxJFjYEdmWD0hO0emSpuT221ax\n5yjgsFxnOA25tSqfS63ntLA8Il4Bvgpcm+fxMLBBtuUA4II8zmhSBO03QE9J00jLV4fmuo0HiLgJ\neJa0Qf3y3G9VLEZojDEdj8UJDZLWBO6v3Ehdun8osEVEHFXtfivHmhMRzW0e71ZIKhy+rjbFGGOW\nGGRxQtNaJH2FtGn3Z500ZJd8s0uar3SsfoakyZKOzXu2OnLMHfOR9g9kUUJjjOlyHOkx3QJJb0bE\nSvl6NdKy1PiIGNmBY65NOqX3A+DWiLixRj1HeowxppU40mNMC8h7eI4AvgvNCkc2K0ipGkKJEfFi\nRMzACtnGGLNYYKfHdEuyYGOPfPKsnnBkSwQp6wklGmOMWUyw02NMEm48JJ/Y+yuwCkk4ErIgZT6B\nVylIuU6+3pYsQkgSStyhM4w2xhjTOrq7OKHppkhaF5gfEf/NG5prCUe2RJCypcfka9K7d9+FuhND\nhw5l6NChre3CGGOWatpDnNAbmU23oHxUPi9pXU3ayHxS3sPzOWBYRMzLwoT/JqUBOS4i9srtxuTP\nk7JDdFxE7CXpZuBPEXG1pK+S8p3tVxp7FHB7RFSm3CjuL/Jfwv79B1rQ0BjTrWjLRmZHekx3oZek\nSaS0Ih8AV0bEOfnepaSlqkk56vMySXW6klrOyVHAqJy/7L80JHDdErgJWBn4vKSROcFpK7puGbNn\nd+jpe2OMWSpwpMeYLiZFehb176F85N0Y063wkfWlCElnSyrnyrpb0iWlz2dJOqYT7dlZ0gJJe5bK\nbivl9qrV7lBJA0qfl5F0mqS/SXpc0nhJ/y/f+2lF2wWSzix9Pk7SL9pvVo3GKsQLJ+ffP2qnfh/K\nvwfm1BvGGGO6CDs9iy/jge1gYbb31YBPle5vR51cTmUWVXk4J9kE+BdwfL26Vfgq8JHS55OB/sBG\nEbElaRmpSEtRqQr9HrCvpFVaOWZbmBsRgyNi8/z7jPboNCLKJ7kcijHGmC7ETs/iy8Nkp4fk7MwA\n5kjqK2k5YEPgSUl/yRGTqZKKDbcDJT0t6YocXVhL0hxJJ2cBvYfVkBl9NUl/yoJ7f5W0bS4fIenK\nHKm4MtsxFXhD0m6Vxko6IbefJuniXLYfKWP91Tl6siLwDeC7ETEPUmLUiPiTpFOBFXK9q3K384BL\ngGOrjFfL7mmSCuXlVyQdnK+vkLSbpI1y/Un5WXy86LLaS5D0nKRTcgToMUmb56jbM5KOzHV6V3sP\n+d6cmm/YGGNMp2KnZzElIv4DfKCk7ltEdf5K0oTZkqQT8zawd46Y7ErKGF/wCeCCiNgkIl4EegMP\nZwG9ccDhud55wNlZcG9/kuhewSeBXSNieGEW8H/ACVVMPj8ito6ITYEVJe2ZTys9Dnw5IgYDHwde\niIgmmesj4qfA2znK8pXSeBcCwyVVJimtZfdDwPaSPkXS1dkxl2+bn+E3gXOzPVuSolfQ4HAVy1vD\nSmM9HxGb575HAfvm/k7M99+l9ntwdMcYYxYTfHpr8eZhYHuS0/Mr4KP58xuk5a8ewGmSdiTpxqwp\nafXc9oWImFDq672IuDNfTwR2z9e7A58sLYH1yREZSPmi3i8bFBEPSQpJ21fYupukHwIrAv1Ikak7\n8r02L69FxFuSrgCOBt4p3apl90PAzsALwMXA4UpZ5F+NiHckPQIcn53JmyLi2dz+7ewIVeO2/Hs6\n0Dsi3gbelvRujiq9DZya9zctfA8R8XJb522MMab9sdOzeFMscW1MciL+BRxHcnpGAcOBVYHNI2KB\npOeAXrltZTTlg9L1fBrevYCtI6J8n+xLNInIZE4Bfl70KWl5UkRmcES8JGlEyY4yzwJrS+oTEW9V\nuV/LOToPmESac7luNbvHAt8B1iLtP9qHFAkaBxAR10p6FPg8cKekIyLigRrjFpQFCSvFCpchvYfV\nqP4eWsiiHTnv339g85WMMWYJpj3ECb28tXjzMOnL+dVIvEbSfCmWavoCL+cv2l2A8jdf5bdorW/V\n0aQoSqokbdacUVm5uB8pLxWkL/gA/iepD8nJKJhDyjRORLxDWoY6T9KyebzV8t4fgPcllR1x5Xav\nAdeTcmTVtTsi/kVyQNaLiOdJkZ8fAGNzvY9FxHMRcT5wS2kObfE6ijYtfQ8dJqYze/YLSPJPF/4M\nGLBOR71eYwxJrX7kyJELf9qCnZ7Fm+mkSM4jFWWvR8SrwB+AIZKmAgcDT5XqtTQ1wtHAlkobcGcA\nR7bQtv8jRVOIiDdIAn9PAHcBj5XqXQ5crLRPZnnSfqBXSJuwp5GWjt7MdS8BpqlhI3PZ5l+RnkVR\nVs/uR4GZ+XocsCbJ+QH4kqQZSnm2PkXDJu1earyn55QqNlRS3Gvpe6jTV/hnCf+ZPfuFpq/VGLNY\nYXFCY7oYtYs4oel6LBBpTGciixOarkDS8Tl6MjVHSbaS9ICkFyrq3ax8hFvpWP0CSSeV7q8q6X1J\nv86fR0qaK2m1Up0OOQKudET/XxXRnpXaod8j1XBsfpSkfRfdWmOMMW3BTo9ZJCRtQ0rWOSgiNiOd\nqvonKXTxmqRCYLEvMIDGIY3ngD1Ln4eRNmwXBCmX1XEVZR3F2RUChW8236Q+EfHbiLi6PYwzxhiz\naNjpMYvKGsArJbHBV7PGEMB1wEH5el/gxoq2bwNPSSqOih9A2rBcZhRwgKSVKweWNFwNQoMXSeoh\naX9Jv8r3j5b093z9MTWkhDgtR6amSCorLzcJkyql0bhJ0mhJ/5D0HUnfz2M+XNgl6RtK4oWTJd0g\nqVcuHyGpibiiMcaYzsdOj1lURpOOoT8t6UI1zsV1P7CjpB7AgSQnqJLrgIOUdHPmAS9V3J8D/B4o\n8owJQNKGJCdpu6yvswD4MmnjcpH6YQfgFUlrkEQKH1RKabF3RGychRpPLo1VODOTJd1XKv8UKV3G\nVqQN3G/lMR8FDsl1/hwRW2URw6dpfNLMGGPMYoB1eswiERFzc6RmR5Ia8XVKiUOD5MQ8RHJ4ekXE\ni1KjPGAB3E1yPGYDf6T6se7zgcmSzqJheWs3YDAwIffZC5gVEbMl9VE6Or8WcA1JrHBH4M8kjaN3\nJF1KEk+8vTTO2RFxdpXxx5QECV8vtZkObJKvN5X0S5KkQG/gnroPzhhjTKdjp8csMpGOrIwFxirl\n+jq0dPuPwE1AkR09KtrOkzSRlF9rI+CLVfp/Q9I1JNHBAgFXRES1BKgPA4eRIi7jSFGXbYBjI2K+\npK1ITtMw4Lv5uh5lQcKgsVhh8XdoFLBXRMyQdCjJ0WoFHSbhYzoJC0Qa07G0hzihnR6zSEhaH1hQ\nSucwCHiepCJNRIzLmjfF0lY1sb5fAQ9ExOuNA0GNOAeYQMOf2fuAmyWdGxH/ldQP+FDOM/YQcBIw\nEpgC7EJKMzFHUm9gxYi4WyklxbOlMRbF8+gDzFISXRxOQ06vNtG//0BmzXp+UbowxpiliqFDhzJ0\n6NCFn0888cTalWtgp8csKn2A8/PprHkkJ+II4E9FhYolo6i8jogngSfrDRIR/5N0E1mFOSKekvRz\nYHTeM/Q+KRL0Iim681FgbFZJfpEGwcAPAbcUG42B75eGOUbScJLzE6R9PE1MqWHiL0iijC+TEsNW\nJkit17bJrdmzHfkxxpj2xuKExnQx1cUJLXRnjDH1sDihWUgVwcAhNeodKun8GveWz8e1p0uaKGmd\nivvzK8T8flSlj50l3VZZ3sI5WJzQGGNMu+HlraWQCsHAefmY9nJ1mtQKKQwj5fnaJC9fVdabm49u\nN0dbQxZlccKfLmJfLaHW6a02ExG/bc/+jDHGtB1HepZOqgkGzpI0RNL4LMr3aN7UC/ARSXdJminp\n9FI/75P2xhARb1RRKK4aVpT0GUlPSXqcJEpYlDcS6ssRpLXz9SE5KjVZ0hWl7ixOaIwxpl2w07N0\n0kQwMJ8qug74Xhbl2x14N9ffjBTV2ZTkYHwkl/8D2FwNGccrWaFiOWiYUib1S4A9I2JLUuqJWgSA\npI2AnwFDs7jf0aU6Fic0xhjTLnh5aymkmmAgcArwUkRMynXegrQRDLiv9PlJYKCk/5GiLBsAV0s6\nKiJ+Lel24Ef5xNXblctbkjYD/hER/8hFVwOHN2PyrsANEfFatu31ivvdQJywcZDJmi/GGNMY6/SY\nmlQRDPxOnepl8b35pD8XmwD/zUfF9wPuTaeM6JcdnnrU2k0/j8bRxRVa0KZbiBP6pJYxxtSnPXR6\nvLy1FCJpfUmfKBUNIungrCFpy1ynj6Sedbp5BthQ0idzlOMbwFnALeWhqrR7mhQp+lj+fFDp3vOk\n6Aw5ElXUuR/YPy89oSQ0WMk5wJE0FifcX9KHizbF/iCSOOEPgAdpECd8ryROuHJE3E1Sgd60mfm0\nlEpxwlYhaan6GTBgnUV4lMYY0zE40rN0UkswcBRwgaQVSBnOd6/SthAMfD1HLK7OS2BvkPbMnCpp\nbEQ8CvSSNIkGMb+7I+Jnko4E7pQ0lxRp6ZP7/jNwSI48/RWYmcd6UtL/kfbczAMmA19rZFQ3Eydc\n0rG4ojFmccTihMZ0MaoqTrikY3FFY0zHIosTGgOSFkg6s/T5OEm/aKaNxQmNMWYpx8tbZmnkPWBf\nSadGxKutaGdxQmOMWYpxpMcsjcwjaQU1EQWUNFDSfUrChPdK+mj5dpX6Fic0xpilBDs9ZmkkgAuB\n4ZIqNxSfD4zKwoTX5M8FFic0xpilGC9vmaWSiHhLKZ3F0cA7pVvbAvvk66uActqNxUaccEnH4orG\nmPbG4oTG1Oc8YBLpqH5BW44UWZzQGGO6GIsTGlMdAeS0FtfTeKnpYRoEEw8mafo0atdGFkmc0GJ+\nxhjT8djpMUsj5bDJr4BVS2VHAYdJmkJyTsrJTY+pOLK+Nk1pTpxwHA1CiC1ty+zZL9S6ZYwxpp2w\nOKExXUwSJ/QSlzHGtAaLEy6lSPqIpJsl/U3SM5LOkbRM6f61+Qj20ZI2yJGKiWrIf9XZ9t4l6TVJ\nt7ag7hclbVjjXlkwcIakA9vRxrYIGDayVdKJknZtL5uMMcZ0LHZ6lgxuBG6MiPWB9Ul5nU4BkDQA\n2DIiBkXEeaSj1TdExBYR8VxLOs+5q9qTM0j7ZVrC3qQj4bU4Ox8P3xv4reonSW0NhYDhKq1o08jW\niBgREfe3kz3GGGM6GDs9izk5kvBORFwJEGkN5PukfSm9SEej18zRkF8AxwDfKnRmJA2X9Nd8/yLl\n7KGS5kg6S9JkYJuKMTeT9EiOHv05Jy5F0hhJp+X+npa0fTWbI2IM8FaVuZwm6Ync7xmStgX2As7I\n9tWMTEXEs8BcoF/ua90cUZog6UFJ6+fyUZLOkzRe0rN10j60RMBwaiFgWM1WldJKSBqSx5wi6VFJ\nvSX1yPP8ay4/vNb8jDHGdDw+sr748ylgYrkgIuYoZQ7/BOmL+LYcDSE7NXMi4uy8FHMAsF1EzJd0\nIWnz7tUkLZlHIuIHVca8EvhORDwk6URgBA3OQc+I2FrSZ4GRwB4tmUSOqOwdERvmzytFxJt5Cey2\niLixmfaDgWci4pVcdAlwZET8XdJWwEXAbvnegIjYXtIngVtJkbJKCgHD6ZJOr7hXCBheLekw4PyI\n2KfS1uw/kk9sXQcMi4hJkvoA75JOjb2en9dywHhJoyPCu5aNMaYLsNOz5NKSzVu7AYOBCdkZ6gXM\nyvfmU8UZUEqy2TciHspFV5COfRcUbSYCrVGgewN4R9KlwB00CPw1x7GSvgasB3wh29gb2A64oYhc\nAcuW2twMEBFPSVq9VsdtFDCsxgbASxExqeg32/lpYBNJw3K9lfI8mjg9vXv3ZeTIkUBTLQpjjDEW\nJ+wuPAnsXy7IjslawLNA/zptBVwREcdXufdOtO24UCHMN59W/PnJkaatSI7YMOC7NERm6nF2jlp9\nAfi9pHVJy7KvFdGtOjZC885hewkYVhtHwPci4t7mGr/11uttGNIYY7oPFifsBkTEfcAKkg4GyBt5\nzyItv7ybq9X6Yr8P2F/Sh3PbfpLWqtcmIt4EXivt1/kK8GCN/us5FCrfz9GZlSPibtJS2ab51hxS\nBKQuEXEbMAE4NCLmAM9JWugMStq0RtNaNrZFwLCWrTOBAZK2yLb0ye/pHuDbyiftJK0naYWqxkid\n8mMRRGNMd8ZOz5LBPsCXJP2NlMzyHaAcvakamYiIp4CfA6MlTQVGA2vUa5M5FDhLScBvM+CkGm2q\n9iFpLPBHYFdJL0rag3Ti7PZsx1jSZmxIe2F+qJYdsf8lDXuLDga+njcIzyDtbWqxjbRNwLDS1kJf\n5wPS3qkLcpvRwPLApaRI3SRJ04GLqRkdi075sQiiMaY7Y3FCY7oYSdG2FbU2jWYRRGPMUoEsTmhM\n5yGpv5Iw5DNKR+dvz0tY50maLmlaPq4+MNd/Tq3TBTLGGNOOeCOzMW3nJtLeqoMAJG1CWuZaIyI2\nyWVrkvSFoPPCOcYYY6rgSI8xbUDSLsD7EfG7oiwippMcnP+Uyl6KiDeKZp1rpTHGmDJ2eoxpGxtT\nIRqZuR7YK6s2nyVpUCfbZYwxpgZe3jKmHYmIfyulxNiVpEP0F0nDcmqOOnROEKh//9boSRpjzOJD\ne4gT+vSWMW1AKSfaiIjYuZl6xwFrR8TRkp4DtoiIVyvqtFEn0hhjui8+vWVMJ5Gzqy8n6RtFmaRN\nJO0kaY38uQdJhPH55vqzgKAxxnQ8jvQY00YkDSClsdiCJBj5PHA3cBiwXK72GPDtiHhf0j+ALatF\nehof7LKWjjHGNEdbIj12eozpYuz0GGNM6/HyllmskXS8pBmSpubTTUPq1B0lad82jvPZLBY4I6eM\nOHNR+1xUJA3MqSiMMcZ0ET69ZToFSdsAnwMGRcS8rEy8XDPN2jLOxsD5wGcj4hlJAo5o73HaiMM3\nxhjThTjSYzqLNYBXImIeQES8GhGzJJ2QUzVMk3RxtYaSBkt6IEdv7pLUP5cfJemJnHT0mlz9h8DJ\nEfFMHici4rel7naWNF7Ss0XUR1JvSX+R9HiOQu2VywdKelLSJTlqdLek5fO9MZJOy7Y/rZyVXlIP\nSWfk8imSDm//R2mMMaYt2OkxncVoYO3sIFwoaadcfn5EbB0RmwIrStqz3EjSMqTIzX4RMQQYBZyS\nb/+YFDkaBHwzl9USDSwYEBHbA18ATs9l7wJ7R8SWJH2dX5XqfyLbuDHwBrBf6V7PiNialDF+ZC77\nOvB6Lt8KOKLIvWWMMaZr8fKW6RQiYq6kwcCOJMfiOkk/Ad6S9CNgRaAfMAO4o9R0A5Ijc29equoB\nvJTvTQWukXQzcHMLTbk52/OUpNVzmYBTsyO2AFizdO+5nF4CkjO1TqmvG0vlhWPzaWATScPy55WA\n9YBn6pvVsBevX7/+LZyKMcZ0H9pDnNBOj+k0sgLfWGBs3tR7JLAJSbDvJUkjgF4VzQTMyNGZSvYE\ndgL2Ao7P+3meALYEam0afq+ib4DhwGrA5hGxIIsI9qpSf36Ffe+Vyou/SwK+FxH3NppEM9Een9Yy\nxpj6DB06lKFDhy78fOKJJ7a6Dy9vmU5B0vqSPlEqGgQ8na9fldQH2L9K05nAh/NGaCQtI2mjfG/t\niHgQ+AkpotIHOBP4qaT1cv0eko6sZVb+3Rd4OTs8u9AQtSnXaXaK+fc9wLfzshyS1pO0QnN9lcUJ\nK38sVmiMMe2DIz2ms+gDnC+pLzAPeJZ0quoN0pLWf0hCfgUBEBEfSNq/1LYncK6kvwFXS1qJ5Eyc\nFxFvAtMlHQNcm52NAG4v91k5BvAH4DZJU4HHgaeq1KmkVl+XkpbAJuXluJeBvZvpq+6t2bOdnN0Y\nY9oDixMa08U0FSdsUsPLX8YYU4HFCU23RtL8LHo4XdIfJVXuD2ppPwskXVn63FPSfyXdmj/vLGnb\n0v0WiR52pTiiMcYYOz1m6WJuRAyOiE2AD2g4xt7qfoCNC00eYA/gn6X7Q4Ht2mylMcaYLsFOj1la\nGUfS2EHSTVnYcLpyVnRJh0k6p6gs6RuSyvo8d5JOhwEcBFyb6w0kOVPH5KhScaqsiehhrn+BpKck\njQZWxxhjTJdhp8csTQgWChp+loZj64dlYcMhwNGS+gHXA5+X1LOoA1xW6us64KAc7dkU+CtARLwA\nXAyck6NK43P9JqKH2flZLyI+CRyKo0PGGNOl+PSWWZpYQdKkfD2OBifmGEnFCaqPkhyRxyTdT3J8\nngaWiYgnc52IiBmS1iFFee6g+aPr1UQPdyRHiCLiP3m8GtTuvn9/CzobY4zFCY1pzNsRMbhcIGln\nkgL01hHxnqQxNAgMXgb8jKQXNKpKf7eSdH+GksQL61FN9LDF+HSWMcbUx+KExjSmmrPRF3gtOzwb\nAtsUNyLiMWAtSnt2Kvr5PXBiRDxR0ecckhhic3aMBQ7IAolrALvUbGARQmOM6XDs9JiliWrhkruB\nZSU9QUpU+kjF/euB8RHxRmU/EfHviLigSp+3AfuUNjJXFSqMiJtIIoxPAJcDD9czffbsF2rfNsYY\ns8hYnNB0ayTdBpwdEWO60IYsTmgRQmOMaSkWJzTdlpIw4QxJkyUdW7q3haRzK+r3lTSTpO3T4Q6P\npJ929BjGGGPq40iPWSqQ9GZErJSvVyPt0RkfESO71LCMpDkR8aEa9xzpMcaYVuJIjzFARLxCSmb6\nXViYNuK20vXkHBWaKKl3Lv+xpGn53im5bJCkRyRNkfTnnPAUSWMkDc7Xq0p6Ll8fmuvdJWmmpNNy\n+ank4/SSrurkx2GMMSbjI+tmqSQinsunpj5cFOXfxwHfjohHJK0IvCvpMyRRwSH5lNfKue4VwHci\n4iFJJwIjgGNpSjk8sxkwiJQGY6ak8yPip5K+U3mc3hhjTOdip8cszVQLe44HzpH0B+DGiPi3pN2B\nURHxHkBEvC5pJaBvRDyU211BOunVHPdFxFsAkp4EBgL/bompvXv3ZeTIkU20KIwxxlic0JiaSFoX\nmBcR/5UafJ+IOF3S7aS8Wg/lKE9rmUfD0nBlJveySOF8Gv6O1V139l4eY4ypj8UJjWlgoVORl7Qu\nAj6YX6UAAAk5SURBVM5vUklaNyKeiIgzgMeBDYB7gcMkrZDr9IuIN4HXSglFvwI8mK+fB7bM18Na\naN/7pTxfTY2XGv1YqNAYY9ofR3rM0kKvnHdrOdJ+misj4pwq9Y6RtAspCvMEcFdEfCBpM+BxSe+R\nMqz/HPgqcHF2hv5BSkoKcBZwvaTDSXm5alEO31wCTJc0MSK+Ur8qzJ7d6kwWxhhjmsFH1o3pYhqO\nrDcq9ZKXMcbUwUfWjWkHJC2QdGXpc09J/5V0a/48oix+WKefj0q6X9ITkqZLOqoj7TbGGFMfL28Z\n05S5wMaSls8nuvYA/tmaDvL+nXnAsRExRVIfYKKk0RHxdPubbIwxpjkc6TGmOneSTnhB0yzsAIMk\nPZxFCL8BC4UPx0q6BXgiImZFxBSAfIz9KeAjnWS/McaYCuz0GNOUAK4DDpK0PLAp8NeKOpsAQ4Ht\ngF9IGpDLNwe+FxEblitLWockWljZjzHGmE7Cy1vGVCEiZmRH5SDSCa3KzXK3RMT7wP8k3Q9sBbwB\nPBYRL5Yr5qWtPwFHF8KFTWncff/+Axd5DsYYszRhcUJjOpZbgTNJEZ3VKu6Vj1ap9HluuZKkZUgO\nz1URcUutgXxSyxhj6mNxQmM6hiLs8nvgxIh4okqdL0paTtKqwM7AhBp9/R54MiLO6wA7jTHGtAI7\nPcY0JQAi4t8RcUGNOtOAB4CHgZMiYlZlhazmPBzYtZTZvS1pL4wxxrQDFic0pgJJC4CrI+KQ/Lkn\nMAt4JCL2kjQCmBMRZ7egr//f3t3HyFXVYRz/PhWhFinxhRfTFQsxEK0INFIhDSASpVhTNDVGNKZC\nlMRgJAGNSGJofEn6hwlpwEjQWhEBtYDSKonWtAkigZY3W4SqCRaL0lUQS4Wo1D7+cc6ss9Nu253d\n7HTuPJ9k0nvv3Jk9v87e3d+ee87vrADeDwzbfvsY5zjXYUTE+KQ4YcTkGKnTU/e7rdMDsBI4fxLb\nFhERXUrSE7F3E67TA2D7XuD5qWp0RESMLUlPxJ4mvU5PRET0XpKeiL2w/Rgwm/3U6bH9HNCq0wN7\nqdMTEREHh9TpiRjbhOv0HKilS5eObHfWooiIiMkpTpjZWxEdJO20fYSkWcAHbV8v6RzgyrbZWxcC\nZwBHAA/V7ZNa53S832xgje2Tx/h6mb0VETFOmb0VMTkmpU4PgKRb6zknSvqTpIvHOG/U49hjZ080\nhoiI6JCenogek+TRd8sAlKUpIiL2IT09MdAk/bdWPX6sVkC+QtK4Loj9vP+StllaSLpRUmZpRUT0\niSQ90SQv2p5r+22UgoIXANeM5w0k7eua+AQwq7Vj+1LbW7pp6CCZ6MDDg13i619Njg2aH183kvRE\nI9l+FrgU+AyM9NJc13pe0hpJZ9ftnZK+LukR4AxJX5K0QdImSTfUcxYD7wC+X3uTpktaL2luff6i\nev4mScvavs5OSV+V9GgtZnjUlP0nHCSa/oM38fWvJscGzY+vG0l6orFs/xGY1pZojDVI5nDKulqn\n2b4PuM72vLpW1gxJC23fATwIfLT2Jv2r9WJJbwCWUaa2nwqcLmlR23vfZ/tU4FfApyY5zIiIOEBJ\neqLpDmRMzy7gzrb98yTdL2kTcC4wZz/vdzqw3vbfbe8GbgHOrs/9x/bddfshSsHDiIjogczeisaQ\n9ILtmW37JwAP2D5K0seAM223bnetBb5i+57219VlJ54C5tr+S63JY9tflrSeUofn4XrueuBKYAhY\nbHtJPX4J8Fbbn2vV/KnHFwMLbV/S0e5chBERXRjv7K1UZI4mGfnmr7e0vgm0xvFsBT5dZ3MN8f9l\nI0a9DphOuQ32nKRXAx8CVtXndgIz2dMGYLmk1wI7KEtXLD/QRo/3oo2IiO4k6YkmmS7pYeBQ4GXg\ne7avBbD9a0lbKaufP0G51dQy0tNie4ekb9XznqEkNC3fBW6Q9BJlodFWEcPtkq6iFCsE+Jntn3a+\nd0RE9FZub0VERMRAyEDmiB6StEDSFkm/l/SFXrdnoiStkDRcB4G3jr1G0i8k/U7SzyUd2cs2dkvS\nkKR1kn4rabOkz9bjTYnvMEkP1MKem+t4tsbEB6UOVy05sbruNyY2AElbJf2mfoYb6rFGxCjpSEmr\nJD1Rr8F3dhNbkp6IHqmFEK8HzqfMELuoARWeV1LiaXcV8EvbJwHrgC9Oeasmxy7gCttzgDOBy+rn\n1Yj4bP8bONf2aZTSCxdImkdD4qsuBx5v229SbAC7gXfV8hutcYtNiXE5cLfttwCnAFvoIrYkPRG9\nMw/4g+2nbL8M/ICyenvfsn0v8HzH4QuBm+r2TcAHprRRk8T2dtuP1u1/UsaGDdGQ+ABsv1Q3D6OM\n+TQNiU/SEPA+4NtthxsRWxux5+/1vo9R0kzgLNsrAWzvsr2DLmJL0hPRO7OAbW37T9O2zEWDHG17\nGEriABzd4/ZMmKTZlN6Q+4FjmhJfvf3zCLAdWGt7I82J71rg84yeXNCU2FoMrJW0UdIn67EmxHg8\n8KyklfX25I2SZtBFbEl6ImKq9fXsiVrK4Hbg8trj0xlP38Zne3e9vTUEzJM0hwbEJ2khMFx76vZV\nIqLvYusw3/ZcSo/WZZLOogGfH6XXcS7wjRrfi5RbW+OOLUlPRO/8GTiubX+oHmuaYUnHAKisUv/X\nHrena5IOoSQ8N9u+qx5uTHwttl+glGBYQDPimw8skvQkcBvwbkk3A9sbENsI28/Uf/8G/IRyC70J\nn9/TwDbbD9b9OyhJ0LhjS9IT0TsbgTdLepOkQ4GPAKt73KbJIEb/Nb2askI9wBLgrs4X9JHvAI/b\nbi8+2Yj4JL2+NftF0quA91DGLfV9fLavtn2c7RMo19k62x8H1tDnsbVImlF7IZF0OPBeYDPN+PyG\ngW2STqyHzqPUUht3bKnTE9FDkhZQZiVMA1bYXraflxzUJN1KWXj1dcAwcA3lL85VwBspS3x82PY/\netXGbkmaD9xD+UXi+riaUsDyR/R/fCdTBoNOq48f2v5arTTe9/G1SDqHspzMoibFJul44MeU78tD\ngFtsL2tKjJJOoQxCfyXwJHAx8ArGGVuSnoiIiBgIub0VERERAyFJT0RERAyEJD0RERExEJL0RERE\nxEBI0hMREREDIUlPREREDIQkPRERETEQkvRERETEQPgfKuEqBnEQyScAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e552668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(TrainSMM, train_label)\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "plt.barh(range(len(TrainSMM.columns.values)), scores)\n",
    "plt.yticks(range(len(TrainSMM.columns.values)), TrainSMM.columns.values, rotation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88243527  0.88270308  0.88305322]\n",
      "Accuracy: 0.88273052398\n",
      "[ 0.00898204  0.00600601  0.01201201]\n",
      "Recall: 0.00900001798205\n",
      "[ 0.375       0.33333333  0.44444444]\n",
      "Precision: 0.384259259259\n"
     ]
    }
   ],
   "source": [
    "#Select the six best features\n",
    "predictors = [\"NumVst\",\"SMMNewsEmail\",\"NoEmail\",\"Distance to SMM\",\"CSI\",\"WarnerNatCtNewsEmail\"]\n",
    "\n",
    "pred = cross_validation.cross_val_score(alg, TrainSMM[predictors], train_label, cv=3)\n",
    "recall = cross_validation.cross_val_score(alg, TrainSMM[predictors], train_label, cv=3, scoring='recall')\n",
    "prec = cross_validation.cross_val_score(alg, TrainSMM[predictors], train_label, cv=3, scoring='precision')\n",
    "\n",
    "print(pred)\n",
    "print(\"Accuracy:\", pred.mean())\n",
    "print(recall)\n",
    "print(\"Recall:\", recall.mean())\n",
    "print(prec)\n",
    "print(\"Precision:\", prec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Classifier(n): \n",
    "    train_pos_idx = random.sample(list(pos.index),800)\n",
    "    train_neg_idx = random.sample(list(neg.index),800)\n",
    "    test_pos_idx = list(set(list(pos.index)) - set(train_pos_idx))\n",
    "    test_neg_idx = list(set(list(neg.index)) - set(train_neg_idx))\n",
    "    train_idx = train_pos_idx + train_neg_idx\n",
    "    test_idx = test_pos_idx + test_neg_idx\n",
    "    \n",
    "    alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "    alg.fit(Train[predictors].loc[train_idx], Train_Labels.loc[train_idx])\n",
    "    pred = alg.predict(Train[predictors].loc[test_idx])\n",
    "    cmpr = pred == Train_Labels.loc[test_idx]\n",
    "    ACC = sum(cmpr)/len(pred)\n",
    "    \n",
    "    Test_Labels = Train_Labels.loc[test_idx]\n",
    "    pos_label = Test_Labels[Test_Labels==1]\n",
    "    neg_label = Test_Labels[Test_Labels==0]\n",
    "\n",
    "    pred_df = pd.Series(pred, index=Test_Labels.index) #convert numpy array to pandas series \n",
    "    pos_pred = pred_df[pos_label.index]\n",
    "    neg_pred = pred_df[neg_label.index]\n",
    "\n",
    "    TP = sum(pos_pred == pos_label)/pos_label.shape[0]\n",
    "    TN = sum(neg_pred == neg_label)/neg_label.shape[0]\n",
    "    Prec = sum(pos_pred == pos_label)/(sum(pos_pred == pos_label)+sum(neg_pred != neg_label))\n",
    "    FM = 2*Prec*TP/(Prec+TP) #F measure aka F1 score\n",
    "    CM = np.matrix([[sum(pos_pred == pos_label), sum(pos_pred != pos_label)], \n",
    "                   [sum(neg_pred != neg_label), sum(neg_pred == neg_label)]])\n",
    "    \n",
    "    return TP,TN,Prec,FM,ACC,CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of TP rate: 0.6025\n",
      "average of TN rate: 0.580679468242\n",
      "average of Precison: 0.0406979230032\n",
      "average of F1 score: 0.076241900288\n",
      "average of Accuracy: 0.581305595409\n"
     ]
    }
   ],
   "source": [
    "#running the Classifier function multiple times to check the performance consistency \n",
    "TP_list = []\n",
    "TN_list = []  \n",
    "Prec_list = []\n",
    "FM_list = []\n",
    "ACC_list = []\n",
    "for i in range(10):\n",
    "    TP,TN,Prec,FM,acc,cm = Classifier(800)\n",
    "    TP_list.append(TP)\n",
    "    TN_list.append(TN)\n",
    "    Prec_list.append(Prec)\n",
    "    FM_list.append(FM)\n",
    "    ACC_list.append(acc)\n",
    "\n",
    "print(\"average of TP rate:\", np.mean(TP_list))\n",
    "print(\"average of TN rate:\", np.mean(TN_list))\n",
    "print(\"average of Precison:\", np.mean(Prec_list))\n",
    "print(\"average of F1 score:\", np.mean(FM_list))\n",
    "print(\"average of Accuracy:\", np.mean(ACC_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance did not improve when using the six best features only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
